{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression, 5050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = 'YOUR_PATH/fNIRS-mental-workload-classifiers/experiments/generic_models/LogisticRegression/binary/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_64vs4_summary_path = '64vs4_summary'\n",
    "scenario_16vs4_summary_path = '16vs4_summary'\n",
    "scenario_4vs4_summary_path = '4vs4_summary'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_64vs4_df = pd.read_csv(os.path.join(root_path, scenario_64vs4_summary_path, 'AllSubjects_summary.csv'))\n",
    "scenario_16vs4_df = pd.read_csv(os.path.join(root_path, scenario_16vs4_summary_path, 'AllSubjects_summary.csv'))\n",
    "scenario_4vs4_df = pd.read_csv(os.path.join(root_path, scenario_4vs4_summary_path, 'AllSubjects_summary.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Binary, Average Accuracy:\n",
      "scenario_64vs4: 67.12919038583178\n",
      "scenario_16vs4: 65.78510436432639\n",
      "scenario_4vs4: 59.827640733712826\n"
     ]
    }
   ],
   "source": [
    "print('LR, Binary, Average Accuracy:')\n",
    "print('scenario_64vs4: {}'.format(np.mean(scenario_64vs4_df.corresponding_test_accuracy.values)))\n",
    "print('scenario_16vs4: {}'.format(np.mean(scenario_16vs4_df.corresponding_test_accuracy.values)))\n",
    "print('scenario_4vs4: {}'.format(np.mean(scenario_4vs4_df.corresponding_test_accuracy.values)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest, 5050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = 'YOUR_PATH/fNIRS-mental-workload-classifiers/experiments/generic_models/RandomForest/binary/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_64vs4_summary_path = '64vs4_summary'\n",
    "scenario_16vs4_summary_path = '16vs4_summary'\n",
    "scenario_4vs4_summary_path = '4vs4_summary'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_64vs4_df = pd.read_csv(os.path.join(root_path, scenario_64vs4_summary_path, 'AllSubjects_summary.csv'))\n",
    "scenario_16vs4_df = pd.read_csv(os.path.join(root_path, scenario_16vs4_summary_path, 'AllSubjects_summary.csv'))\n",
    "scenario_4vs4_df = pd.read_csv(os.path.join(root_path, scenario_4vs4_summary_path, 'AllSubjects_summary.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Binary, Average Accuracy:\n",
      "scenario_64vs4: 72.24462365591397\n",
      "scenario_16vs4: 66.94339025932953\n",
      "scenario_4vs4: 61.47216951296649\n"
     ]
    }
   ],
   "source": [
    "print('LR, Binary, Average Accuracy:')\n",
    "print('scenario_64vs4: {}'.format(np.mean(scenario_64vs4_df.corresponding_test_accuracy.values)))\n",
    "print('scenario_16vs4: {}'.format(np.mean(scenario_16vs4_df.corresponding_test_accuracy.values)))\n",
    "print('scenario_4vs4: {}'.format(np.mean(scenario_4vs4_df.corresponding_test_accuracy.values)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepConvNet, 5050, 600epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = 'YOUR_PATH/fNIRS-mental-workload-classifiers/experiments/generic_models/DeepConvNet/binary/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_64vs4_summary_path = '64vs4_summary'\n",
    "scenario_16vs4_summary_path = '16vs4_summary'\n",
    "scenario_4vs4_summary_path = '4vs4_summary'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DCN_scenario_64vs4_df = pd.read_csv(os.path.join(root_path, scenario_64vs4_summary_path, 'AllSubjects_summary.csv'))\n",
    "DCN_scenario_16vs4_df = pd.read_csv(os.path.join(root_path, scenario_16vs4_summary_path, 'AllSubjects_summary.csv'))\n",
    "DCN_scenario_4vs4_df = pd.read_csv(os.path.join(root_path, scenario_4vs4_summary_path, 'AllSubjects_summary.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepConvNet, Binary, Average Accuracy:\n",
      "scenario_64vs4: 70.6752055660974\n",
      "scenario_16vs4: 64.80075901328274\n",
      "scenario_4vs4: 56.586021505376344\n"
     ]
    }
   ],
   "source": [
    "print('DeepConvNet, Binary, Average Accuracy:')\n",
    "print('scenario_64vs4: {}'.format(np.mean(DCN_scenario_64vs4_df.corresponding_test_accuracy.values)))\n",
    "print('scenario_16vs4: {}'.format(np.mean(DCN_scenario_16vs4_df.corresponding_test_accuracy.values)))\n",
    "print('scenario_4vs4: {}'.format(np.mean(DCN_scenario_4vs4_df.corresponding_test_accuracy.values)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EEGNet, 5050, 600epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = 'YOUR_PATH/fNIRS-mental-workload-classifiers/experiments/generic_models/EEGNet/binary/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_64vs4_summary_path = '64vs4_summary'\n",
    "scenario_16vs4_summary_path = '16vs4_summary'\n",
    "scenario_4vs4_summary_path = '4vs4_summary'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "EEGNet_scenario_64vs4_df = pd.read_csv(os.path.join(root_path, scenario_64vs4_summary_path, 'AllSubjects_summary.csv'))\n",
    "EEGNet_scenario_16vs4_df = pd.read_csv(os.path.join(root_path, scenario_16vs4_summary_path, 'AllSubjects_summary.csv'))\n",
    "EEGNet_scenario_4vs4_df = pd.read_csv(os.path.join(root_path, scenario_4vs4_summary_path, 'AllSubjects_summary.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEGNet, Binary, Average Accuracy:\n",
      "scenario_64vs4: 69.75806451612904\n",
      "scenario_16vs4: 66.25948766603418\n",
      "scenario_4vs4: 55.123339658444024\n"
     ]
    }
   ],
   "source": [
    "print('EEGNet, Binary, Average Accuracy:')\n",
    "print('scenario_64vs4: {}'.format(np.mean(EEGNet_scenario_64vs4_df.corresponding_test_accuracy.values)))\n",
    "print('scenario_16vs4: {}'.format(np.mean(EEGNet_scenario_16vs4_df.corresponding_test_accuracy.values)))\n",
    "print('scenario_4vs4: {}'.format(np.mean(EEGNet_scenario_4vs4_df.corresponding_test_accuracy.values)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
